# @package _global_

# Fine-tune Qwen2.5-0.5B on MRPC (paraphrase detection)
# Run: python src/train.py experiment=finetune_mrpc

defaults:
  - override /data: glue
  - override /model: qwen2_finetune
  - override /callbacks: default
  - override /trainer: gpu

tags: ["qwen2", "mrpc", "finetune"]

seed: 42

trainer:
  min_epochs: 1
  max_epochs: 5
  gradient_clip_val: 1.0
  precision: "bf16-mixed"
  accumulate_grad_batches: 2

data:
  task_name: "mrpc"
  model_name: "Qwen/Qwen2.5-0.5B"
  max_length: 256
  batch_size: 16
  num_workers: 4

model:
  model_name: "Qwen/Qwen2.5-0.5B"
  num_labels: 2
  task_metric: "f1"
  finetune_mode: "full"
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 100

logger:
  wandb:
    name: "finetune_mrpc_lr${model.learning_rate}_ep${trainer.max_epochs}"
    group: "finetune"
    job_type: "finetune"

callbacks:
  model_checkpoint:
    monitor: "val/f1"
    mode: "max"
  early_stopping:
    monitor: "val/f1"
    mode: "max"
    patience: 5
