# @package _global_

# Fine-tune Qwen2.5-0.5B on CoLA (GLUE linguistic acceptability)
# Run: python src/train.py experiment=finetune_cola

defaults:
  - override /data: glue
  - override /model: qwen2_finetune
  - override /callbacks: default
  - override /trainer: gpu

tags: ["qwen2", "cola", "finetune"]

seed: 42

trainer:
  min_epochs: 1
  max_epochs: 5
  gradient_clip_val: 1.0
  precision: "bf16-mixed"
  accumulate_grad_batches: 2

data:
  task_name: "cola"
  model_name: "Qwen/Qwen2.5-0.5B"
  max_length: 128
  batch_size: 16
  num_workers: 4

model:
  model_name: "Qwen/Qwen2.5-0.5B"
  num_labels: 2
  task_metric: "matthews_correlation"
  finetune_mode: "full"
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 100

callbacks:
  model_checkpoint:
    monitor: "val/matthews_correlation"
    mode: "max"
  early_stopping:
    monitor: "val/matthews_correlation"
    mode: "max"
    patience: 5
